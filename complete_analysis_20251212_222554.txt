====================================================================================================
COMPLETE HYPERPARAMETER TUNING ANALYSIS - ALL 5 CONFIGURATIONS
====================================================================================================

                                  Configuration  Learning Rate  Batch Size  Hidden Dim  Dropout  Epochs  LSTM Layers Optimizer  Best Val Acc  Final F1  Final Precision  Final Recall  Training Time (min)
                 Config 1: Baseline (10 epochs)         0.0010          64         128     0.30      10            2      Adam        0.7042    0.6426           0.6863        0.6042                 4.46
                 Config 2: Extended (15 epochs)         0.0005          32         256     0.40      15            2      Adam        0.6949    0.6479           0.6656        0.6311                17.93
                     Config 3: Fast (20 epochs)         0.0100         128          64     0.20      20            2   RMSprop        0.7039    0.6757           0.6572        0.6953                 4.22
     Config 4: Deep Model (3 layers, 12 epochs)         0.0008          64         128     0.35      12            3      Adam        0.7053    0.6650           0.6605        0.6694                 7.27
Config 5: Regularized (High dropout, 10 epochs)         0.0010          64         128     0.50      10            2       SGD        0.5432    0.0000           0.0000        0.0000                 3.98

BEST CONFIGURATION FOR PRODUCTION:
Config 4: Deep Model (3 LSTM layers, 12 epochs)
Accuracy: 70.53%
F1-Score: 66.50%

KEY FINDING:
High dropout (0.5) combined with SGD optimizer causes training failure.
Optimal dropout range is 0.2-0.35 with Adam optimizer.
